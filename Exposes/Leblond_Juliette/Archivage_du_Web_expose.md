# L’Archivage du Web en France :

![alt text](stockage_web.jpg)

**Photographie des machines de stockage de archives du Web de la BNF situées dans le sous-sol du site François Mitterrand**         
*Crédits : Morgan Tual, Le Monde, 26/10/2016*

># Sommaire: 
>- [Introduction](#naissance-dune-problematique-complexe.)
>- [Étendre les prérogatives de la BnF et de l’INA : les enjeux politiques et juridiques de l'élargissement du dépôt légal](#etendre-les-prerogatives-de-la-bnf-et-de-lina--les-enjeux-politiques-et-juridiques-de-lelargissement-du-depot-legal)
>- [Les enjeux techniques de la collecte : préserver, conserver et valoriser le patrimoine français](#les-enjeux-techniques-de-la-collecte--preserver-conserver-et-valoriser-le-patrimoine-francais)
>- [Utiliser les archives du Web : les enjeux méthodologiques des recherches à partir du Web français](#utiliser-les-archives-du-web--les-enjeux-methodologiques-des-recherches-a-partir-du-web-francais)
>- [Bibliographie](####Bibliographie)


  
# L'archivage du Web : naissance d'une problématique complexe. 

Inventé par le chercheur Tim Berners-Lee en 1989, le Web s’est imposé comme un outil quotidien support de données variées auxquelles ont accès et/ou produisent des milliards d’utilisateurs à travers le monde. 
Le Web n’a pas cessé d’évoluer ces vingt dernières années, son architecture a été transformée et son emploi démocratisé[^1].
D’abord envisagé pour des chercheurs, c’est un public large qui s’est investi de ce service en participant à l’évolution de ce contenu et de son architecture. 
L’analyse du web permet de comprendre les sociétés contemporaines, ses enjeux et ses évolutions. Dans les années 1990, des historiens ont commencé à s’intéresser au Web. Les premiers travaux ont exposé les freins liés à cette pratique : l’instabilité et la pérennité des données pose un véritable défi pour les historiens. Les données sont en effet volatiles, évoluent voire disparaissent. 
Leur disparition est matérialisée par le renvoi d’un message d’erreur 404. Ainsi, tout comme une charte de l’époque médiévale est altérée par le temps et dépend des conditions matérielles de conservation, les sites Web se transforment ou disparaissent tous les quatre-vingts jours en moyenne[^2].
De ce fait, la conservation au sens du latin *conservare* prend tout son sens. Pourtant, la question de comment récupérer et stocker sans altérer le nombre important de données qui structurent une page Web se pose. Sous quelles formes conserver ces données ?

![alt text](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExbzhrOGYxeGVkdXNob3doYmtuenE3M2V6cG5nc3JidWk4Z2ZzcTZtZCZlcD12MV9naWZzX3NlYXJjaCZjdD1n/xTiN0L7EW5trfOvEk0/giphy.webp) <!-- -->


Conscients de ces enjeux, c’est d’abord un acteur privé, Brewster Khale, qui se lance dans un projet visant à archiver le Web en créant la
fondation *Internet Archive* en 1996. C’est par l’utilisation de robots *crawler*[^3] qu’il se lance dans une première copie de sites variés, qui concernent l’élection présidentielle de 1996 opposant Bill Clinton à Bob Dole, avant de continuer en élargissant son œuvre. 

<img src="768px-Internet_Archive_logo_and_wordmark.jpg" alt="alt text" width="400" height="400">       <img src="Khale.jpg" alt="alt text" width="400" height="400">    

Il est possible de saisir les enjeux de construction de la mémoire nationale qui sous-tendent ce premier projet dont l’entendu est important. 
En effet, une élection mobilise un large panel d’acteurs de la vie politique et civile qui sont au fait de l’ensemble de moyens qui permettent de toucher l’électorat, *a fortioti* dans un pays où le vote n'est ni figé, ni prévisible dans les *swing states*. 

Khale est souvent présenté comme « un visionnaire, humaniste militant »[^4]. En effet, dans la stratégie médiatique qu’il tente d’imposer pour publiciser son projet, il évoque sa volonté de constituer « une bibliothèque numérique universelle »[^5], à l’image de la Bibliothèque d’Alexandrie. Ce n’est donc pas anodin qu’une copie des données ait été envoyée à la bibliothèque d’Alexandrie.

Dès lors, alors que des enjeux patrimoniaux et mémoriaux sous-tendent la question de l’archivage du Web, comment laisser un acteur privé archiver le Web d’états ? On voit donc se lancer dans des projets similaires des bibliothèques nationales de plusieurs pays. 
Ainsi, la bibliothèque Nationale de Suède est la première à entreprendre en 1997 une collecte à partir des sites répertoriés sur un nom de domaine national. 

Cette entreprise importante passe rapidement par la collaboration, c’est ainsi qu’à l’échelle européenne, le *Network European Deposit Library* est initié dans le cadre du programme de recherche et développement de l’UE reposant sur la collaboration de neuf bibliothèques, trois maisons d’éditions ainsi que des entreprises d’informatiques.

En France, la question de l’archivage des productions mobilise directement la **Bibliothèque nationale de France** (BNF) et l’**Institut national de l’audiovisuel** (INA) qui sont en charge du dépôt légal. Cette entreprise a une visée mémorielle. 
Il semble donc naturel que la charge d’archiver le Web français leur ait été confié. 
Dans quelle mesure la nature évolutive et complexe du Web a-t-elle modifié les pratiques de conservation et de valorisation de la BnF et de l’INA ? 
Comment les acteurs politiques mais aussi la BnF et l’INA envisagent-elles ces archives de nature numérique ?
Quelles sont les modalités et les acteurs de l’archivage du Web français ? En quoi l’archivage du Web a-t-il également modifié les pratiques des historiensdont le travail est pris en compte dans le processus d’archivage ? 

# Étendre les prérogatives de la BnF et de l’INA : les enjeux politiques et juridiques de l'élargissement du dépôt légal. 

> Notons d'ores et déjà que dans la législation, une confusion sémantique est faite entre Internet et le Web. Ainsi, si la loi dispose de l'élargissement du dépôt légal à Internet, il s'agit en réalité du dépôt légal du Web. 

La France connaît depuis 1537 le dépôt légal des documents. Il implique que l’ensemble des documents édités qui sont produits ou qui touchent la France, soient données à des autorités de conservation qui sont la Bibliothèque nationale de France et l’Institut national de
l’audiovisuel. 
Ce dépôt a une véritable dimension patrimoniale. Il a été élargi à mesure où les pratiques de production ont évolué.
Ainsi, par exemple, à la suite de la surpression du monopole étatique sur les contenus audiovisuels dans les années 1980, il est nécessaire, dans une visée patrimoniale et mémorielle de préserver les programmes nouveaux d’acteurs inédits sur le champ audiovisuel national. C’est ainsi que les législateurs s’emparent de la question en [1992](https://www.legifrance.gouv.fr/jorf/id/JORFTEXT000000723108#:~:text=1er.,la%20disposition%20d'un%20public). Ainsi, il suffirait d'étendre ce dépôt légal au Web afin de faciliter toute entreprise d'archivage du Web. Pourtant, nous verrons que ce processus a été complexe. 

## Une vision économique du Web : les prémices de l’archivage du Web
  
Les débats politiques se concentrent d’abord sur la question des internautes français avec l’établissement du programme d’action gouvernemental de la société d’information, à l’initiative du gouvernement L. Jospin en 1997.
Celui-ci n’aborde aucunement les enjeux patrimoniaux et mémoriels sur le WEB et pour cause, le projet est piloté par le ministère de l’Économie, des Finances et de l’Industrie. Le Web est d’abord envisagé comme « un facteur de croissance économique »[^6]. 
Il y a derrière une volonté de protection des internautes et des données qu’ils produisent, notamment en ce qui concerne les industriels.

Parallèlement, l’État confie l’archivage de ses premiers sites[^7] à la fondation privée d’Internet Archives en 1996. 
Se pose donc le souci que des informations politiques relatives à la mémoire d’un pays se retrouvent hébergés sur des serveurs aux États-Unis. 
Cela fait prendre conscience aux acteurs politiques que cette activité devrait être le fruit de l’État lui-même. Il est alors convenu qu’une prochaine loi sur la société de l’information prendra en considération ces éléments.

Ainsi, quand le projet de loi sur la société de l'information (LSI) est initié par le gouvernement à l’Assemblée Nationale en mars 2000, toujours pilotée par L. Fabius, ministre de l’économie et des finances, une réflexion autour de l’archivage du Web français émerge. 
Il serait question d’élargir le dépôt légal au Web afin d’en archiver ses contenus[^8]. 
Mais, du fait d’un calendrier parlementaire serré, ces discussions n’aboutissent pas à des amendements en ce sens[^9]. 
Pourtant ces discussions permettent de lever quelques questionnements. D’abord, un manquement est remarqué : le Web ne possède pas de personnalité juridique. 
Ensuite, il est question de la méthode très concrète pour comprendre ce qu’il faudrait archiver et sur quelle unité se baser, c’est-à-dire, faudrait-il archiver la page, le site ou bien uniquement le document ? 
Le paradoxe important est lié au fait que le Web est considéré comme une source de croissance économique. Il ne faudrait donc pas que le dépôt légal soit à la charge des acteurs producteurs, mais davantage celle de l’État. Enfin, c’est la questionplus large du droit d’auteur qui est abordée. Comment copier des contenus en respectant le droit à la propriété intellectuelle et le droit d'auteur ? 

## L’établissement difficile de la loi Dadvsi témoin de visions différentes autour du Web. 
  
C’est à l’échelle européenne que les choses s’accélèrent. [Une directive du Parlement européen](https://www.legifrance.gouv.fr/jorf/id/JORFTEXT000000523361) et du Conseil du 22 mai 2001[^11] relative à l’harmonisation de certains aspects du droit d’auteur et des droits des voisins dans la société de l’information oblige le gouvernement français à réouvrir les débats.
En vue de l'adaptation de la directive dans le droit national français, un Conseil supérieur de la propriété littéraire et artistique est créé.
Un premier paradoxe apparait : pourquoi, alors que le dépôt légal est historiquement lié aux institutions publiques, archivistiques et documentaires, aucun acteur du secteur archivistique, des bibliothèques ou de la documentation n’y siège ? La loi est présentée par le gouvernement et par les médias comme liée à Internet, justifiant ainsi une exclusion de ces acteurs. 

![alt text](images.jpg) <!-- -->

Le texte qui émerge de cette discussion entre le Comité et le Conseil des ministres prévoit que seuls la BNF et l’INA sont habilités à copier « les contenus en ligne selon un mode d’échantillonnage permettant de constituer progressivement une mémoire collective »[^12], sans que les bibliothèques ne puissent y avoir accès[^13]. 
Une association archives-bibliothèque-documentation est alors créée en 2002 afin de revendiquer leur position sur ce texte. Elle prend une ampleur assez importante, ce qui fait que les forces politiques s’emparent de leurs revendications au Parlement. 
Ils souhaitent véritablement trouver un équilibre entre une protection des productions, et une ouverture pour un accès culturel. 
L’idée est d’autoriser les exceptions prévues par le Parlement européen à savoir, permettre à l’ensemble des institutions
patrimoniales françaises (musées, bibliothèques, archives française) de reproduire à des fins de conservation et de communication et à des buts pédagogiques. Ce qu’on observe donc c’est que ces acteurs patrimoniaux ont conscience que le Web est nouveau patrimoine, qu’il constitue un enjeu mémoriel et qu’il pourra être exploité à des fins pédagogiques et scientifiques.

Après des débats qui aboutissent à l’établissement d'une commission mixte paritaire et la saisine du Conseil constitutionnel, la loi Dadvsi
est promulguée au Bulletin Officiel le 3 août 2006. Elle aura opposé, non pas des couleurs politiques, mais bien plusieurs élus locaux très
investis des revendications portées par ces associations. Par rapport au projet originel du gouvernement, la loi protège strictement les
producteurs, mais élargit les exceptions qu'elle avait envisagée. 
Ainsi, les bibliothèques, musées et services d’archives sont autorisés à reproduire certaines données à condition qu’elles n’aient pas de visées commerciales. La BNF et l’INA sont considérés comme les instances légitimes à l’archivage des sites web et documents dématérialisés (à savoir logiciels et bases de données). La BNF est aussi « dépositaire des fichier numériques des éditeurs qui sont mis à disposition d’organismes agréés pour la  réalisation d’éditions adaptées aux publics handicapés »[^14].

## Définir le « Web français » et un périmètre d’action : les ressorts du décret d’application de 2011.
 
Il faut attendre le décret d’application de décembre 2011[^15], pour qu'une définition  du Web soit intégrée clairement à un texte juridique et pour comprendre quelles données peuvent être touchées par l'archivage. 
Ainsi, c'est un ensemble « des signes, signaux, écrits et images, sons ou messages de toute nature faisant l’objet d’une communication par voie électronique et public »[^10]., c’est-à-dire qu’un grand nombre de fichiers à destination du public, sont visés par la collecte. 
Avec cette définition, on note unepremière limitation des moissonnages. Il est interdit de collecter ce qui touche les correspondances privées et les espaces privées qui sont hébergés sur le web.
Ensuite, il a été question d’établir quels sites peuvent concrètement faire l'objet d'un moissonnage. On rappelle que le dépôt légal touche des productions françaises ou qui sont populaires sur le territoire. 
Dans ce sillage, il a été décrété que les sites sont considérés comme français s'ils sont « enregistrés sous le nom de domaine .fr ou tout autre nom de domaine enregistré auprès des organismes français chargés de la gestion de ces noms, et/ou produits sur le territoire français ou enregistrés par une personne domiciliée en France »[^16]. 
Afin d’affiner ce protocole, la BNF fait appel à l’Association française pour le nommage internet en coopération (Afnic) qui est chargé d’attribuer les noms de domaine. Le robot peut aussi en réalité crawler les sites qui ont un autre nom de domaine comme *.com* ou *.org* si leur contenu a été produit en France. Le décret prévoit également que ces informations soient vérifiées à *posteriori* de la collecte. Ainsi, une marge de manœuvre est laissée à la fois à la BnF et aux éditeurs qui peuvent faire recours.

Ainsi, malgré une définition difficile, la loi et son décret d’application traduisent parfaitement la vision du Web comme faisant partie intégrante du patrimoine français qu’il faut conserver et qui
constitue une entrée de premier plan dans la compréhension des sociétés contemporaines.

# Les enjeux techniques de la collecte : préserver, conserver et valoriser le patrimoine français.

Une fois le cadre légal établi, comment concrètement archiver ce qu’on considère comme le Web français? 
Il faut aussi mener une réflexion autour de ce qu’on considère comme étant une archive du Web. Comme le montre Schafer, Muisiani et Borelli dans un article en 2016, elle ne constitue aucunement un objet statique et elle dépend des modalités de la collecte, des technologies mais aussi « des modèles et paradigmes qui sous-tendent l’archivage »[^17] et la conservation. 
Ce qu’on entend par là, c’est que le Web ne va pas être archivé de la même manière s’il a pour but de servir un modèle documentaire, mémoriel ou encore temporel. Pour autant, pour Niels Ole Finneman, il y aurait tout de même une convergence des méthodes déployées par les États et les institutions compétentes autour de la question d’archives du web. Il s'agirait d'obtenir par la collecte un contenu original et ses évolutions, ainsi qu’une visée d’exploitation et d’analyse par des spécialistes. On retrouve totalement cette vision avec la loi Dadvsi.

![alt text](BNF.jpg)         <img src="Logo_INA.jpg" alt="alt text" width="250" height="250">  <!-- -->

Plus largement, comment dépasser les problèmes techniques définis par Howard Besser[^18] liés à la préservation du numérique, à savoir le stockage et la maintenance concrète des archives, l’intégrité du document et de son contexte, la pratique d’acquisition, et la perte d’information lors de la migration d’un support vers un autre ?


## Le moissonnage large des sites : vers une représentativité du web ?

La BnF s'est dotée d’une cinquantaine de robots moissonneurs qui sont chargés deux fois par an, d’identifier et de copier automatiquement les fichiers en ligne dans les dispositions du décret d’application de 2011. 
On pourrait comparer cette démarche à celle d’un utilisateur qui cliquerait sur tous les sites qu’ils rencontrent. La finalité de cette démarche n'est pas exhaustive mais bien représentative.
La BNF utilise la technologie *Heritrix* développée par Internet Archive. Trèsconcrètement, ce robot moissonneur fait ce qu’on appelle du *web scrapping*. Il est chargé de collecter le code source, la mise en page CSS, les fichiers HTML ou encore les fichiers binaires (photos, vidéos…). Ensuite, l'indexation permet  de séparer les objets archivés - c'est-à-dire les images, le texte, les vidéos... -  et les informations liées à la structure de l’affichage - paragraphes, formats tabulaires, styles…. Chaque information est associée à une clé d’identification. Ainsi, quand l’utilisateur fera une requête, il est associe des éléments entre eux. 

En fonction des collectes, un paramétrage des robots est établi : on précise la profondeur, la date, la qualité et l’étendu de la
collection. Collecter en profondeur signifie que les robots récupèrent les liens entrants qui sont à l’intérieur d’un site et collecter en
largeur que ce sont les liens sortants vers d’autres sites qui sont récupérés. 

Pour autant, ce moissonnage est limité pour des raisons pratiques liées au stockage. Il peut également être limité par les producteurs qui peuvent tout à fait intégrer dans leur site un protocole d’exclusion des robos qu’on appelle *robots.txt*[^19].

Aussi, à l’image d’une conservation classique, il y a un contrôle pendant de la collecte autour de la qualité à l’aide d’un échantillon
sur lequel on analyse le poids, la taille ou la répartition des collections par types de fichier. Il y a aussi des technologies qui
permettent d’éviter le dédoublement et de vérifier que l’URL est encore en ligne. 

Une fois collectés, ces données sont stockés sur un format propre aux archives du Web, des fichiers (W)ARC sur lesquels on retrouve le fichier compressé issu du moissonnage  ainsi qu’un ensemble de métadonnées comme la date de capture, le poids….
Ensuite, ces fichiers (W)ARC sont stockés sur des disques appelés *Petabox* d’une capacité d’entre 60 et 120 téraoctets. C’est bien à partir de ces disques qu’on peut consulter les archives. Pour autant, la BNF est avant tout une instance de conservation. 
Dès lors, ces fichiers (W)ARC sont aussi copiés et sauvegardés dans un système appelé SPAR (Système de préservation et d’archivage réparti). 
C’est à cette étape qu’intervient la normalisation des données en vue de l’interopérabilité. En effet, il faut que ces fichiers suivent le modèle de *l’Open Archival Information System*. Ainsi, chaque fichier doit recevoir un identifiant et une validation et chacun fait l’objet de migration et d’émulation afin de pouvoir rester lisibles sur le long terme. Le signalement de données repose sur des normes établies à l’échelle internationale depuis 2008. Il est véritablement nécessaire à l’interrogation des données. 


> ### La patrimonialisation des archives à la BNF :
> ![alt text](spar.jpg)<!-- -->
>
> Comme on peut le voir, le SPAR est support de conservation de l'ensemble de contenus numériques de la BNF qu'il s'agisse des collections nummérisées ou bien de l'administratif ou encore de l'archivage du WEB. 
> Source : SPAR, [site Web de la BNF](https://www.bnf.fr/fr/spar-systeme-de-preservation-et-darchivage-reparti).
> 

C’est environ deux semaines après une capture, qu’il est possible de naviguer à l’aide de la *Wayback machine* (indexation et navigation pour la recherche URL) ou *NutchWAX* (indexation plein texte et navigation pour la recherche par mot). 
Un système de navigation entre Web vivant et archives a aussi été mis en place par les bibliothécaires de la BNF. Ces outils sont à disposition des chercheurs entre les murs de la BNF ou de ses partenaires régionaux.


On retrouve la même logique en ce qui concerne l’INA, à la différence que la masse de sites est différente puisque ce sont environ 14 000
sites[^20] qui sont quotidiennement moissonnés.

On voit donc bien une adaptation claire du processus d’archivage et de conservation de ces sites issus du Web, mais qui reste tout de même dans les logiques des institutions patrimoniales.

## Les collectes ciblées : un processus sélectif révélant les enjeux d’une construction mémorielle :

À côté de ce moissonnage large, il existe un modèle de selection ciblé.
C’est d’ailleurs par ce modèle que la BNF a débuté son expérimentation
d’archivage du Web, bien avant le cadre légal, au début des années 2000.

Des premières collectes ciblées sont effectuées entre 2002 et 2004 lors des élections présidentielles, régionales et européennes. Pour autant, elle a impliqué da manière active dans ce processus *Internet Archive*. En 2002, ce  sont au total 3500 sites qui ont été moissonnés. 
La collecte est  thématique, ce qui a nécessité un paramétrage précis qui est passé par la création d’un Comité comprenant 18
acteurs de différents départements de la BNF notamment appartenant au département d’Histoire et Philosophie, de Droit et Science politique et celui du dépôt légal. Ces acteurs, aux visions différentes ont réussi à établir ce que Catherine Lupovici appelle une typologie de sites, afin de cibler les sites. 
Ce sont des productions d’organisation politiques, de candidats, de médias satiriques, humoristique ou traditionnels, d’institutions officielles, d’observatoire, du net-citoyenneté ou encore d’enseignement et recherche qui seront moissonnés. C’est tout de même une méthode de prospection qui est établie et qui évolue au fur et à mesure de la campagne, notamment au moment de l’entre-deux-tours où une forte mobilisation du Web est constaté notamment par l’investiture du Web par le « front-républicain ». Ce sont en 2002, 50 000 URL qui ont été sélectionnées couvrant à la fois les élections des primaires à la fin du deuxième tour. Les captures des sites validés ont été effectuée à plusieurs fréquences en fonction de l’importance de participation des sites dans l’élection. Par exemple, les sites des candidats ou des partis ont été collectés plusieurs fois par semaine, ceux des médias,
une fois par semaine pendant le premier tour et trois fois pendant le
second tour[^21].

Cette pratique s’est ensuite imposée et élargie notamment du fait des évolutions dépôt légal et des pratiques de campagne électoral et l’utilisation des médias sociaux.

La collecte ciblée repose donc sur le choix de sites par des bibliothécaires qui vont par ailleurs suivre la même logique de paramétrage des robots *crawler*. Ils sont donc des acteurs essentiels dans la construction de la mémoire collective. Ils sont amenés à
réfléchir sans cesse à la granularité et à la temporalité des productions. En effet, les sites des médias vont requérir une capture hebdomadaire en surface, alors que le site de Matignon, une collecte biannuelle en profondeur. Il existe deux types de collectes ciblés qui répondent chacun à des enjeux différents et qui impliquent une intervention forte des conservateurs et bibliothécaires.

D’une part, il y a les collectes dites « projets » qui peuvent être le fruit d’une collaboration institutionnelle entre des partenaires variés et qui va être sensible à la fois à l’actualité et mais aussi à des thématiques qui peuvent répondre aux besoins de la recherche actuelle. Citons en exemple une collecte a liée au Covid au moment du début de la pandémie et du confinement. Son but est de comprendre le caractère global, social et économique de la crise. Ce projet a été mené par la BNF et quinze partenaires régionaux. C’est ensuite un
travail de valorisation qui a été fait puisqu’un parcours guidé a été mis en ligne par les conservateurs et des chercheurs. Parmi ces collectes projets, on compte notamment ce qu’on appelle les collectes dites « d’urgences » qui ont la même dynamique qu’une conservation d’urgence dans le sens où il s’agit de capturer rapidement les sites à collecter à des dates précises ou qui pourraient disparaître. On voit donc une prise de conscience dans ces pratiques de la nature très éphémères des données sur le Web. Ainsi par exemple, en
2015, l’INA et la BNF ont lancé une collecte d’urgence autour de la [couverture médiatique des attentats](https://www.ina.fr/ina-eclaire-actu/video/5609665_001_041/attentats-a-paris-les-annonces-de-bernard-cazeneuve) et des réactions des internautes, à la suite que des internautes comme le chercheur de l’université de York Nick Ruest aient collectés des Tweets. Ces collectes ont véritablement
nécessité une adaptation rapide et donné lieu à une approche quantitative des traces numériques des attentats.

D’autre part, il existe des collectes dites courantes qui sont également ciblées. Il s’agit de collectes sur des sites de référence qui 
permettent de garantir « une continuité patrimoniale »[^22]. On retrouve donc ciblés par ces collectes des périodiques, de la musique, ou encore des encyclopédies. Elles répondent aux politiques documentaires dechacun des départements de la BNF et de l’INA. 

L'ensemble de ces collectes font ensuite l'objet de constructions de dossiers thématiques, consultables sur les sites physiques de la BNF et de l'INA. Tout comme pour les collections physiques, ces archives numériques font l'objet d'une réflexion autour de leur valorisation. Il y a en effet une masse importante de données qui sont traitées et choisies en fonction des thèmes. On reste ici dans une valorisation classique, dans le sillage des expositions que les institutions organisent ou des dossiers thématiques sur la base de leurs archives nummérisées qu'on peut retrouver sur [Gallica](https://gallica.bnf.fr/accueil/fr/content/accueil-fr?mode=desktop)

Malgré une continuité dans le processus traditionnel de patrimonialisation, l'archivage du Web français a nécessité une formation des acteurs (archivistes, bibliothécaires et conservateurs) au enjeux du Web et aux outils de la collecte. Dès le début des années 2000, à l'initative de Julien Massanès - conservateur- et Jean-Noël Jeanneney- directeur de la BNF, c'est une dizaine, puis une cinquantaine de personnes qui ont été initiées aux collectes. 

> #### Quelques chiffres relatifs à la collecte de la BNF de 2023:
> [Communication officielle de la Bibliothèque Nationale de France en février 2024.](https://www.bnf.fr/sites/default/files/2024-02/CP_bilan_archivage_du_web_2023.pdf) 
>
>1. Moissonnage large :
>   - 5 731 808 domaines moissonnés.
>   - 2 200 URL collectées par domaine.
>   - 3 173 362 231 URL sauvegardées.
>     
>2. Deux collectes d'urgence
>  - Les Skyblogs, entreprise de la BNF et de l'INA depuis 2022.
>     - 12 607 289 blogs.
>     - 1 873 992 846 URL.
>  -  Pages personnelles Orange :
>      - 298 188 sites.
>      - 26 094 982 URL. 


# Utiliser les archives du Web : les enjeux méthodologiques des recherches à partir du Web français.

Le processus d'archivage du Web est techniquement complexe et nécessite aussi des moyens humains et économiques importants. Ainsi, c'est par la collaboration que des normes sont trouvées afin de faciliter l'appréhension de ces données par les chercheurs habilités à exploiter les archives sans cesse en extention. Ceux-ci sont en réalité intégrés dans le processus. Par ailleurs, ces archives d'une nature nouvelle nécessite de prendre en compte les spécificités dans les recherches.  

  ## Une collaboration poussée entre acteurs : entre standardisation et approfondissements.

Ce travail nécessite donc un ensemble d’acteurs importants à différenteséchelles, régionales, nationales mais aussi internationale. Dès 2003, laBNF a fait partie des fondateurs du [consortium international pour la préservation de l’internet (IIPC)](https://netpreserve.org/) qui compte aujourd’hui plus de cinquante membres. Ce consortium inité par Jean-Noël Jeanneney et Brewster Kahle est critiqué, mais rapidement la *British Library*, les bibliothèques des pays scandinaves et la bibliothèque du Congrès le rejoigne. Il y a une véritable prise de conscience d'une entreprise, soit nationale, mais collective dans les mêmes temporalités. S'unir permet donc d'échanger autour de problématiques similaires. 

On note d’abord une collaboration technique puisque les logiciels de moissonnages et permettant la consultation sont le fruit d’une création commune. Ainsi, le format WARC (ISO 28500) qui caractérise le stockage et la préservation des archives est issu de ce *consortium*. Ceci permet également de réduire les coûts économiques qu’engendrent ces nouvelles pratiques. C’est aussi plus globalement une
méthode de travail qui est promu autour notamment des questions de métadonnées qui permettent la description des collectes. En vue de
l’interopérabilité, la question de la standardisation est nécessaire.

Or, il n’existe aucun cadre particulier et donc il a fallu créer certaines normes que les institutions, à travers des luttes
d’influences, tentent d’imposer aux éditeurs de sites. Ceci permet de gagner du temps mais aussi de faciliter les moissonnages. 
Ces normes visent à termes une possibilité d’échanges mais aussi à améliorer les conditions d’accès et de préservation. L’un des objectifs à terme est de « d’encourager le développement et l’utilisation d’outils communs, de techniques et des normes de création d’archives internationales [^23]». En règle générale, le développement de la *FAIR data*[^24], une des missions des institutions patrimoniales française est aussi de réfléchir au partage des corpus[^25].

De plus, les institutions publiques, dans une volonté de s’adapter aux évolutions du Web sont appelés à collaborer avec des plateformes qui
hébergent des contenus. Ainsi, en mai 2023, l’INA a signé un partenariat important avec le réseau social TikTok. En effet, ce réseau social
permettrait un accès  privilégié à des tendances culturelles et audiovisuelles matérialisées notamment par les *trends* qui correspondent à des tendances qui naissent de partages de contenu entre internautes. Ceci nécessite en réalité une révision des technologies de *crawling*, ce qui a pour conséquence le recours à une aide de la part des acteurs de la plateforme.

Enfin, le développement de la collaboration en vue de davantage documenter les collectes passe par une écoute du public concerné par la
production à savoir les chercheurs qui seront les premiers à appréhender ces masses importantes de données. Ainsi, régulièrement, des entretiens oraux sont organisés et publiés par l’INA et par la BNF afin de répondre à la transparence de la collecte, mais aussi pour discuter des limites techniques. Généralement, ces entretiens ont lieu en amont de la constitution des corpus ce qui permettrait selon V. Schafer et S. Gebeil de s’assurer de la faisabilité d’accès aux données. Ceci est d’autant plus important quand les projets sont transnationaux comme pour la collecte du Web liée au COVDI-19, où les temporalités ne sont pas identiques en fonction des pays. On peut aussi noter un nombre important de journées d’études réunissant des acteurs variés dans une même idée autour des thèmes de la patrimonialisation du Web français. Par exemple, dès 2010, l’INA propose plusieurs ateliers de recherche méthodologiques du dépôt légal du Web média.

Cette collaboration donne aussi lieu à des premiers travaux autour des archives associant archivistes et historiens. On peut par exemple citer le projet porté par Valérie Schafer « Web90 » autour du patrimoine et des mémoires du Web dans les années 1990.

## Un nouveau rapport à l’analyse et au traitement des archives du Web français : entre continuité et adaptation des historiens.

Comme nous l’avons précédemment souligné, l’archivage du Web français est avant tout pensé pour un public de chercheur. Ils sont intégrés pleinement dans le processus de création d’archives et sont invités à consulter ces archives entre les murs de la BNF et de l'INA et de leurs partenaires. Pour autant, comment produire un discours historique à partir de ces corpus ? Comment les historiens prennent-ils en considération les spécificités de ces collections ? Quelles sont les nouvelles contraintes et limites ?

> #### Quelques travaux méthodologiques et historiques à partir du Web français  :
>
> - [Sophie Gebeil,"Les mémoires de l'immigration maghrébine sur le Web français (1996-2013)"," Migrations Sociétés", n°151(1), pp.165-180](https://shs.cairn.info/revue-migrations-societe-2014-1-page-165?lang=fr) 
> - [Sophie Gebeil, "Web vivant et web archivé, aux sources de l’histoire nativement numérique",*Le web :
> source et archive*, Soutenu par le GIS CollEx-Persée et porté par le Service Commun de Documentation
> de l’Université de Lille et la Bibliothèque nationale de France, en partenariat avec le GERiiCO de
> l’Université de Lille, Sciences Po et le Campus Condorcet, le réseau ResPaDon se donne pour objectif
> d’ouvrir un espace de dialogue entre chercheurs et professionnels des bibliothèques et des archives.,
> Apr 2023, Lille, France.](https://amu.hal.science/hal-04058328/document)
> - [Quentin Lobbé, "Continuity and discontinuity in web archives: a multi-level reconstruction of the firsttuesday community through > persistences, continuity spaces and web cernes", *Internet Histories*, 2023.](https://www.tandfonline.com/doi/full/10.1080/24701475.2023.2254050)

Notons d’abord que les archives du Web sont lacunaires et incomplètes notamment du fait des enjeux autour de la représentativité du Web qui définissent les collectes biannuelles de la BNF et de l’INA. Il peut manquer des pages entières notamment quand il s’agit des premières
expérimentations d’Internet Archive récupérées par la BNF au début des années 2000. De plus, malgré la mise en place d’une navigation entre « web mort » et « web vivant » permis par les archivistes de la BNF, l’interconnectivité entre les sites est limitée du fait de temporalité différentes de moissonnage. La « re-médiation »[^26] induite par le système de collecte puis d’indexation a pour conséquence qu’au sein de l’archive coexistent des éléments qui ont pu ne pas coexister sur le Web[^27].

Le rapport de l’historien à l’archive se retrouve modifié. En effet, archiver revient à stabiliser mais aussi à figer un contenu. Or, une
archive du Web est vivante et remodelée selon des mécanismes qui respectent à la fois des contraintes économiques, juridiques mais aussi techniques. Il faut donc que l’historien appréhende ces contraintes et processus dans son analyse. Ceci requiert notamment selon Sophie Gebeil de passer par une ouverture interdisciplinaire aux sciences informatiques et aux sciences de l’information afin de parvenir au mieux à établir une méthodologie précise de critique documentaire. Elle propose par ailleurs une méthodologie à partir du site de l’association
Génériques [www.generiques.org](http://www.generiques.org) dans le but d’étudier les mémoires de l’immigration. Pour se faire, l’historienne a débuté le dépouillement des corpus archivé, par la compréhension de sa constitution. Elle a ensuite confronté plusieurs versions archivées et à des sources complémentaires. Dans le cas de son étude, elle a par exemple mené des enquêtes orales des acteurs de
l’association.

La complexité des données permet aux historiens d’aborder les corpus à travers différentes approches. Ils peuvent adopter une approche dite « par cas »[^28] c’est-à-dire envisager un corpus restreint, voire à partir d’un seul site. Ainsi, l’historien peut se concentrer sur
l’historicité du contenu à travers sa production, ses producteurs et sa temporalité. Mais au contraire, la masse de données peut à l’inverse
favoriser une lecture distante. On peut citer ici la thèse de Quentin Lobbé qui a travaillé sur les e-diasporas à partir des forums [Yabiladi.com](https://www.yabiladi.com/forum/) qui a été archivé par l’INA. Il expose sa méthode comme étant « une
exploration désagrégée de corpus d’archives Web » à partir « de sous-ensembles cohérents et auto-suffisant d’une page Web »[^29]. Il a abordé la question de la représentativité des diasporas en ligne notamment à travers une analyse réseau. Enfin, des projets qui reposent
souvent sur une collaboration entre chercheurs, ont envisagé une approche mixte, c’est-à-dire abordant à la fois lecture distante et
étude qualitative en fonction des spécificités des découvertes et des axes d’études. Par exemple, le projet *Polyvocal Interpretation of
Contested Colonial Heritage (2021/2023)* a entre autres envisagé la médiatisation de la Marche pour l’égalité et contre le Racisme de 1983 propose une analyse distante des données archivées par l’INA permise par des outils d’extractions et sémantiques performants afin de comprendre les processus d’évolution de la médiatisation qui peuvent traduire des demandes de reconfigurations mémorielles dans les politiques culturelles.



### Bibliographie : 
#### Monographies 
Musiani, Francesca, Camille Paloque-Berges, Valérie Schafer, et Benjamin G. Thierry. *Qu’est-ce qu’une archive du Web* Encyclopédie Numérique. Marseille: Open Edition Press, 2019.

Revel, Jacques et Passeron, Jean-Claude. *Penser par cas*. Éditions de l’École des hautes études en sciences sociales, 2005.

Sitts, Maxine K. *Handbook for Digital Projects: A Management Tool for Preservation and Access*. Andover, Massachusetts: Northeast Document Conservation Center, 2000.

#### Chapitre d’ouvrage : 
Musiani, Francesca. « Archivage du Web, un enjeu de gouvernance d’Internet ». In Clarrise Bardidot, Esther Dehoux, Emilien Ruiz (dir). *La fabrique numérique des corpus en sciences humaines et sociales*, Presses Universitaires du Septentrion., 2022.


#### Mémoires et Thèses 
Chaimbault, Thomas. *L’archivage du Web* . École nationale supérieure des sciences de l’information et des bibliothèques, 2008. https://www.enssib.fr/bibliotheque-numerique/documents/1730-l-archivage-du-web.pdf.

Zielinski, Océane. * L’archivage du web français par la Bibliothèque nationale de France : une nouvelle approche des missions de la BNF?* Mémoire, Angers, 2019. https://dune.univ-angers.fr/fichiers/18009927/2019HMSIB11183/fichier/11183F.pdf.


#### Articles de revues 
Foatelli, Alexandre. « L’archivage du web : un outil pour comprendre internet ». *La Revue des médias*, 2016. https://larevuedesmedias.ina.fr/larchivage-du-web-un-outil-pour-comprendre-internet#.

Genevievepiejut. « Quand le web devient une archive : la construction du cadre légal ». *WebCorpora : explorer les archives d'Internet*, 15 juin 2017. https://doi.org/10.58079/VA8I.

Illien, Gildas. « Le dépôt légal de l’internet en pratique : : les moissonneurs du web ». *Bulletin des bibliothèques de France (BBF)* 6 (2008): 20 27.

Genin, Christine. « 20 ans d’archivage du web: Une nouvelle aventure pour les bibliothécaires ». *Biens Symboliques / Symbolic Goods* 2 (2018). https://doi.org/10.4000/bssg.271.

Lupovici, Catherine. « L’archivage de l’Internet à la Bibliothèque nationale de France ». *Matériaux pour l’histoire de notre temps* 79, no 1 (2005): 82 84. https://doi.org/10.3406/mat.2005.1049.

Mussou, Claude. « Et le Web devint archive : enjeux et défis »: *Le Temps des médias* n° 19, no 2 (16 novembre 2012): 259 66. https://doi.org/10.3917/tdm.019.0259.


#### Articles de presse : 

Croquet, Pierre. « Comment les archivistes de la BNF sauvegardent la mémoire du confinement sur Internet ». *Le Monde*, 15 mai 2020. https://www.lemonde.fr/pixels/article/2020/05/15/a-la-bnf-les-archivistes-du-web-sauvegardent-l-internet-francais-du-confinement_6039704_4408996.html.

Morgane, Tual. « Vingt ans d’archivage du Web: les coulisses d’un projet titanesque ». *Le Monde*, 26 octobre 2016.https://www.lemonde.fr/pixels/article/2016/10/26/vingt-ans-d-archivage-du-web-un-projet-titanesque_5020433_4408996.html 

### Sitographie : 
Bibliothèque nationale de France, "Dépôt légal du Web". https://www.bnf.fr/fr/depot-legal-du-web 
INA, "Les Web média", https://www.ina.fr/institut-national-audiovisuel/collections-audiovisuelles/le-web-media 

[^1]: On peut penser au « Web 2.0 » (T. O’Reilly) dont des interfaces
    d’échanges simplifiés ont permis de rendre accessible à un public
    non initié de naviguer sur les Web.

[^2]: Tim-Berners Lee, interviewé par le Journal Le Monde en avril 2016.
    <https://www.lemonde.fr/pixels/article/2016/10/26/vingt-ans-d-archivage-du-web-un-projet-titanesque_5020433_4408996.html>

[^3]: Robot qui explore et indexe automatiquement des pages web en
    collectant les données.

[^4]: Claude Mussou, « Et le Web devient archives : enjeux et défis »,
    *Le Temps des Médias*, n°19(2), pp.256-259.
    <https://doi.org/10.3917/tdm.019.0259>.

[^5]: Tim-Berners Lee, interviewé par le Journal Le Monde en avril 2016.
    <https://www.lemonde.fr/pixels/article/2016/10/26/vingt-ans-d-archivage-du-web-un-projet-titanesque_5020433_4408996.html>

[^6]: G. Piejut, « Quand le web devient une archive : la construction du
    cadre légal », *Web Cropora : explorer les archives de l’internet à
    la BnF,* 2024. <https://webcorpora.hypotheses.org/283>

[^7]: Celui de Matignon et de l’Élysée.

[^8]: G. Piejut, « Quand le web devient une archive : la construction du
    cadre légal », *Web Cropora : explorer les archives de l’internet à
    la BnF,* 2024. <https://webcorpora.hypotheses.org/283>

[^9]: Le calendrier parlementaire français tente de répartir l’analyse
    des projets et propositions de lois, eux-mêmes dépendants des
    groupes parlementaires qui ont une répartition de la parole. Cette
    loi est débattue à l’approche des élections présidentielles de 2002.

[^10]: Cette définition est gardée telle quelle dans la loi du 21 juin
    2004 relative à la confiance dans l’économie
    numérique.<https://www.legifrance.gouv.fr/codes/section_lc/JORFTEXT000000801164/LEGISCTA000006117684/>

[^11]: Directive 2001/29/CE

[^12]: Présentation par Jean-Jacques Aillagon, ministre de la Culture et
    de la Communication, du projet de loi au conseil des ministres du 12
    novembre 2003.

[^13]: On note en réalité une seconde exception liée à la copie et la
    traduction des données pour un public handicapé.

[^14]: Informations sur la page Web « dépôts légal » de la Bibliothèque
    nationale de France. <https://www.bnf.fr/fr/le-depot-legal>

[^15]: Décret n°2011-1904.
    <https://www.legifrance.gouv.fr/loda/id/LEGIARTI000025002917/2011-12-22/>

[^16]: G. Illen, « Le dépôt légal de l’internet en pratique : les
    moissonneurs du web », *Bulletin des bibliothèques de France,* 2008,
    n°6, pp. 20-27.
    <https://bbf.enssib.fr/consulter/bbf-2008-06-0020-004>

[^17]: F. Musiani. Archivage du Web, un enjeu de gouvernance
    (d'Internet). Presses Universitaires du Septentrion. *Clarisse
    Bardiot, Esther Dehoux, Emilien Ruiz (dir.) La fabrique numérique
    des corpus en sciences humaines et sociales*,
    2022. [⟨halshs-03912550⟩](https://shs.hal.science/halshs-03912550v1)

[^18]: M. Sitts, “ Digital Longevity”, *Digital Projects, a Management
    Tool for Preservation and Access,* Northeast document conservation
    center, 2000.
    <https://www.nedcc.org/assets/media/documents/dman.pdf>

[^19]: Informations trouvées en parcourant le site » robots.txt,
    Introduction au protocole d’exclusion des robots ».
    <https://robots-txt.com/>

[^20]: A. Foatelli, « L’archivage du web : un outil pour comprendre
    internet », *La Revue des médias*, INA, 2016.
    <https://larevuedesmedias.ina.fr/larchivage-du-web-un-outil-pour-comprendre-internet>

[^21]: C. Lupovici, « L’archivage de l’Internet à la Bibliothèqe
    nationale de France. Matériaux pour l’histoire de notre temps, 2005,
    n°79, pp. 82-84. <https://doi.org/10.3406/mat.2005.1049>

[^22]: G. Illen, « Le dépôt légal de l’internet en pratique : les
    moissonneurs du web », *Bulletin des bibliothèques de France,* 2008,
    n°6, pp. 20-27.
    <https://bbf.enssib.fr/consulter/bbf-2008-06-0020-004>

[^23]: T. Chaimbault, dossier documentaire intitulé : « l’archivage du
    Web », école nationale supérieure des sciences de l’information et
    des bibliothèqes.
    <https://www.enssib.fr/bibliotheque-numerique/documents/1730-l-archivage-du-web.pdf>

[^24]: Mons, 2018, *findable*, *accessible*, *interopable* et
    *reusable*.

[^25]: Voir le contrat de performance de la BNF 2022-2026.
    <https://www.bnf.fr/fr/contrat-de-performance>

[^26]: N. Brügger, « L’historiographie de sites Web : quelques enjeux
    fondamentaux », *Le Temps des médias*, 2012, n°18, 1, pp. 159-169.

[^27]: S. Gebeil, « Quand l’historien rencontre les archives du Web »,
    *Revue de la BNF*, 2016, n°53, 2, pp185-191.
    <https://doi.org/10.3917/rbnf.053.0185>.

[^28]: J.-C. Passeron, J. Revel, *Penser par cas*, Paris, l’École des
    Hautes Études en Sciences Sociales, 2005.

[^29]: Voir l’explication de sa thèse sur son site Web :
    <https://qlobbe.net/bio.html>

